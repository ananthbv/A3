private static Integer MAX_ITERATIONS = 100;
	private static Integer NUM_INTERVALS = 20;
-----------------------------------------------

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100
Value Iteration,113247,79365,276,113,64,64,59,66,76,59,65,64,67,65,76,53,57,59,73,61
Policy Iteration,701833,369006,5740,412,85,59,64,60,60,62,77,68,73,59,61,76,58,70,70,72
Q Learning,2140,940,599,317,727,231,235,539,841,267,272,530,521,121,225,216,532,732,183,159

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100
Value Iteration,206,93,115,201,185,220,256,295,327,362,392,432,464,510,545,577,625,667,670,711
Policy Iteration,101,128,321,376,428,484,517,631,569,636,691,755,810,875,934,959,996,1037,1084,1150
Q Learning,80,73,87,90,105,108,124,118,128,147,149,154,152,172,160,178,174,170,188,185

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100
Value Iteration Rewards,-113145.0,-79263.0,-174.0,-11.0,38.0,38.0,43.0,36.0,26.0,43.0,37.0,38.0,35.0,37.0,26.0,49.0,45.0,43.0,29.0,41.0
Policy Iteration Rewards,-701731.0,-368904.0,-5638.0,-310.0,17.0,43.0,38.0,42.0,42.0,40.0,25.0,34.0,29.0,43.0,41.0,26.0,44.0,32.0,32.0,30.0
Q Learning Rewards,-2038.0,-838.0,-497.0,-215.0,-625.0,-129.0,-133.0,-437.0,-739.0,-165.0,-170.0,-428.0,-419.0,-19.0,-123.0,-114.0,-430.0,-630.0,-81.0,-57.0





private static Integer MAX_ITERATIONS = 1000;
	private static Integer NUM_INTERVALS = 20;
-----------------------------------------------
	

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000
Value Iteration,57,61,64,59,64,63,62,65,81,59,63,71,76,62,58,62,59,69,59,70
Policy Iteration,52,60,60,62,65,64,55,56,60,61,56,60,57,63,62,72,55,70,70,65
Q Learning,497,182,159,254,157,131,146,176,170,182,224,203,215,224,145,126,172,155,166,311

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000
Value Iteration,553,852,1160,1420,1770,2076,-1822,-1520,-1165,-814,-204,188,252,583,900,1363,1741,1988,-1981,-1617
Policy Iteration,738,1154,1742,-1964,-1402,-866,-297,307,853,1372,-2141,-1611,-1177,-656,-58,465,938,1640,2088,-1575
Q Learning,197,195,221,275,276,318,320,324,396,396,427,453,478,492,506,536,539,571,588,612

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000
Value Iteration Rewards,45.0,41.0,38.0,43.0,38.0,39.0,40.0,37.0,21.0,43.0,39.0,31.0,26.0,40.0,44.0,40.0,43.0,33.0,43.0,32.0
Policy Iteration Rewards,50.0,42.0,42.0,40.0,37.0,38.0,47.0,46.0,42.0,41.0,46.0,42.0,45.0,39.0,40.0,30.0,47.0,32.0,32.0,37.0
Q Learning Rewards,-395.0,-80.0,-57.0,-152.0,-55.0,-29.0,-44.0,-74.0,-68.0,-80.0,-122.0,-101.0,-113.0,-122.0,-43.0,-24.0,-70.0,-53.0,-64.0,-209.0






Only for Q Learning
----------------
private static Integer MAX_ITERATIONS = 10000;
	private static Integer NUM_INTERVALS = 100;

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000
Value Iteration,
Policy Iteration,
Q Learning,618,174,220,247,234,108,122,154,162,179,118,230,103,153,165,113,144,145,130,252,135,188,171,170,134,122,130,114,105,193,122,237,163,107,133,159,227,131,228,122,137,255,89,124,301,145,119,105,97,248,163,161,114,139,118,180,114,141,130,147,146,150,153,142,127,144,128,106,159,162,282,118,158,100,127,140,144,134,96,199,175,179,182,179,213,78,134,151,139,180,117,110,162,119,183,117,186,80,216,88

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000
Value Iteration,
Policy Iteration,
Q Learning,416,289,354,389,433,422,547,523,553,610,648,708,713,801,851,915,962,979,993,1043,1124,1152,1139,1250,1255,1292,1383,1415,1488,1526,1660,1662,1713,1747,1760,1775,1800,1820,1932,2018,2091,2051,-2140,2112,-2072,-2097,-1982,-1962,-1846,-1779,-1859,-1810,-1810,-1647,-1572,-1594,-1547,-1550,-1557,-1434,-1524,-1433,-1376,-1194,-1148,-1198,-1076,-1043,-889,-1067,-913,-889,-755,-927,-860,-760,-683,-573,-636,-564,-573,-239,-549,-400,-261,-336,-200,-420,-81,-186,-100,-84,67,-13,23,368,373,239,203,422

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-516.0,-72.0,-118.0,-145.0,-132.0,-6.0,-20.0,-52.0,-60.0,-77.0,-16.0,-128.0,-1.0,-51.0,-63.0,-11.0,-42.0,-43.0,-28.0,-150.0,-33.0,-86.0,-69.0,-68.0,-32.0,-20.0,-28.0,-12.0,-3.0,-91.0,-20.0,-135.0,-61.0,-5.0,-31.0,-57.0,-125.0,-29.0,-126.0,-20.0,-35.0,-153.0,13.0,-22.0,-199.0,-43.0,-17.0,-3.0,5.0,-146.0,-61.0,-59.0,-12.0,-37.0,-16.0,-78.0,-12.0,-39.0,-28.0,-45.0,-44.0,-48.0,-51.0,-40.0,-25.0,-42.0,-26.0,-4.0,-57.0,-60.0,-180.0,-16.0,-56.0,2.0,-25.0,-38.0,-42.0,-32.0,6.0,-97.0,-73.0,-77.0,-80.0,-77.0,-111.0,24.0,-32.0,-49.0,-37.0,-78.0,-15.0,-8.0,-60.0,-17.0,-81.0,-15.0,-84.0,22.0,-114.0,14.0







------------------------------------------


private static Integer MAX_ITERATIONS = 10000;
	private static Integer NUM_INTERVALS = 20;


The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9500,10000
Value Iteration,59,63,64,69,69,57,57,57,59,65,73,72,69,71,55,76,64,57,52,57
Policy Iteration,79,57,55,66,62,67,68,63,56,63,74,72,59,55,71,66,69,59,60,52
Q Learning,151,209,148,185,321,108,299,133,132,198,106,170,130,108,99,165,119,82,268,117

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9500,10000
Value Iteration,-350,-1515,-2023,1285,637,480,-215,-1536,-1873,1883,379,72,206,-1652,-1329,-1932,63,-429,14,454
Policy Iteration,1660,-1508,-182,1378,-836,-99,1343,-1448,-262,1264,-1762,1147,1273,-792,-613,536,-895,-1095,1061,-1581
Q Learning,653,664,988,1148,1285,1668,1692,1947,-2046,-1877,-1677,-1345,-1191,-972,-775,-452,-452,-131,132,204

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9500,10000
Value Iteration Rewards,43.0,39.0,38.0,33.0,33.0,45.0,45.0,45.0,43.0,37.0,29.0,30.0,33.0,31.0,47.0,26.0,38.0,45.0,50.0,45.0
Policy Iteration Rewards,23.0,45.0,47.0,36.0,40.0,35.0,34.0,39.0,46.0,39.0,28.0,30.0,43.0,47.0,31.0,36.0,33.0,43.0,42.0,50.0
Q Learning Rewards,-49.0,-107.0,-46.0,-83.0,-219.0,-6.0,-197.0,-31.0,-30.0,-96.0,-4.0,-68.0,-28.0,-6.0,3.0,-63.0,-17.0,20.0,-166.0,-15.0





-------------------------

QLearning with GreedyQ
The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9500,10000
Value Iteration,
Policy Iteration,
Q Learning,109,83,108,111,76,75,92,95,81,57,72,103,109,67,82,82,79,89,77,92

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9500,10000
Value Iteration,
Policy Iteration,
Q Learning,575,481,725,749,878,882,1102,1170,1314,1375,1516,1803,1989,1800,1856,-1901,-1841,-1945,-1471,-1712

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9500,10000
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-7.0,19.0,-6.0,-9.0,26.0,27.0,10.0,7.0,21.0,45.0,30.0,-1.0,-7.0,35.0,20.0,20.0,23.0,13.0,25.0,10.0
